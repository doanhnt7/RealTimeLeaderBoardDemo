{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Leaderboard Analysis - Local Mode\n",
        "\n",
        "Notebook này chạy PySpark ở local mode để gen snapshot leaderboard data từ parquet file.\n",
        "\n",
        "## Pipeline:\n",
        "1. Đọc data từ Parquet file\n",
        "2. Transform thành Score objects với event time\n",
        "3. Tính tổng điểm trong sliding window (5 phút gần nhất)\n",
        "4. Tính TopN với retraction logic\n",
        "5. Snapshot mỗi 7 phút theo event time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, deque\n",
        "import math\n",
        "\n",
        "# PySpark imports\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping existing SparkContext...\n",
            "❌ Error creating Spark session: 'JavaPackage' object is not callable\n",
            "\n",
            "Troubleshooting steps:\n",
            "1. Make sure Java is installed and JAVA_HOME is set\n",
            "2. Check Java version: java -version\n",
            "3. Check JAVA_HOME: echo $JAVA_HOME (Linux/Mac) or echo %JAVA_HOME% (Windows)\n",
            "4. Try restarting Jupyter kernel completely\n",
            "5. Check if any other Spark processes are running\n",
            "\n",
            "Java version check: java version \"17.0.12\" 2024-07-16 LTS\n",
            "Java(TM) SE Runtime Environment (build 17.0.12+8-LTS-286)\n",
            "Java HotSpot(TM) 64-Bit Server VM (build 17.0.12+8-LTS-286, mixed mode, sharing)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create Spark Session with proper error handling and cleanup\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Create Spark session with proper cleanup and error handling\"\"\"\n",
        "    try:\n",
        "        # First, try to stop any existing SparkContext\n",
        "        try:\n",
        "            sc = SparkContext._active_spark_context\n",
        "            if sc is not None:\n",
        "                print(\"Stopping existing SparkContext...\")\n",
        "                sc.stop()\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Create new Spark session\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"LeaderBoardAnalysis\") \\\n",
        "            .master(\"local[2]\") \\\n",
        "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "            .getOrCreate()\n",
        "        \n",
        "        print(f\"✅ Spark session created successfully!\")\n",
        "        print(f\"Spark version: {spark.version}\")\n",
        "        print(f\"Spark UI: http://localhost:4040\")\n",
        "        return spark\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating Spark session: {e}\")\n",
        "        print(\"\\nTroubleshooting steps:\")\n",
        "        print(\"1. Make sure Java is installed and JAVA_HOME is set\")\n",
        "        print(\"2. Check Java version: java -version\")\n",
        "        print(\"3. Check JAVA_HOME: echo $JAVA_HOME (Linux/Mac) or echo %JAVA_HOME% (Windows)\")\n",
        "        print(\"4. Try restarting Jupyter kernel completely\")\n",
        "        print(\"5. Check if any other Spark processes are running\")\n",
        "        \n",
        "        # Try to get more detailed error information\n",
        "        try:\n",
        "            import subprocess\n",
        "            result = subprocess.run(['java', '-version'], capture_output=True, text=True)\n",
        "            print(f\"\\nJava version check: {result.stderr}\")\n",
        "        except:\n",
        "            print(\"\\nJava not found in PATH\")\n",
        "        \n",
        "        return None\n",
        "\n",
        "# Create the Spark session\n",
        "spark = create_spark_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Java installation and environment\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def check_java_installation():\n",
        "    \"\"\"Check Java installation and environment variables\"\"\"\n",
        "    print(\"=== JAVA INSTALLATION CHECK ===\")\n",
        "    \n",
        "    # Check JAVA_HOME\n",
        "    java_home = os.environ.get('JAVA_HOME')\n",
        "    if java_home:\n",
        "        print(f\"✅ JAVA_HOME is set to: {java_home}\")\n",
        "    else:\n",
        "        print(\"❌ JAVA_HOME is not set\")\n",
        "    \n",
        "    # Check Java version\n",
        "    try:\n",
        "        result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=10)\n",
        "        if result.returncode == 0:\n",
        "            print(\"✅ Java is installed:\")\n",
        "            print(result.stderr)\n",
        "        else:\n",
        "            print(\"❌ Java command failed\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"❌ Java command timed out\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ Java not found in PATH\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking Java: {e}\")\n",
        "    \n",
        "    # Check Python environment\n",
        "    print(f\"\\n=== PYTHON ENVIRONMENT ===\")\n",
        "    print(f\"Python version: {sys.version}\")\n",
        "    print(f\"Python executable: {sys.executable}\")\n",
        "    \n",
        "    # Check if we can import pyspark\n",
        "    try:\n",
        "        import pyspark\n",
        "        print(f\"✅ PySpark version: {pyspark.__version__}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"❌ PySpark import error: {e}\")\n",
        "\n",
        "# Run the check\n",
        "check_java_installation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hướng dẫn khắc phục lỗi Spark\n",
        "\n",
        "### Lỗi 1: `'JavaPackage' object is not callable`\n",
        "**Nguyên nhân:** PySpark không thể kết nối với Java\n",
        "\n",
        "**Giải pháp:**\n",
        "1. **Cài đặt Java 8 hoặc 11:**\n",
        "   - Tải Java từ: https://adoptium.net/\n",
        "   - Chọn Java 8 hoặc 11 (không dùng Java 17+ vì có thể gây xung đột)\n",
        "\n",
        "2. **Thiết lập JAVA_HOME:**\n",
        "   - Windows: `set JAVA_HOME=C:\\Program Files\\Eclipse Adoptium\\jdk-8.0.xxx`\n",
        "   - Linux/Mac: `export JAVA_HOME=/usr/lib/jvm/java-8-openjdk`\n",
        "\n",
        "3. **Thêm Java vào PATH:**\n",
        "   - Windows: Thêm `%JAVA_HOME%\\bin` vào PATH\n",
        "   - Linux/Mac: Thêm `$JAVA_HOME/bin` vào PATH\n",
        "\n",
        "### Lỗi 2: `Cannot run multiple SparkContexts at once`\n",
        "**Nguyên nhân:** Đã có SparkContext đang chạy từ lần thử trước\n",
        "\n",
        "**Giải pháp:**\n",
        "1. **Restart Jupyter kernel hoàn toàn:**\n",
        "   - Kernel → Restart & Clear Output\n",
        "   - Hoặc Kernel → Shutdown → Start lại\n",
        "\n",
        "2. **Kiểm tra processes đang chạy:**\n",
        "   - Windows: `tasklist | findstr java`\n",
        "   - Linux/Mac: `ps aux | grep java`\n",
        "\n",
        "3. **Kill processes nếu cần:**\n",
        "   - Windows: `taskkill /f /im java.exe`\n",
        "   - Linux/Mac: `pkill -f java`\n",
        "\n",
        "### Các bước khắc phục nhanh:\n",
        "1. **Restart Jupyter kernel**\n",
        "2. **Chạy lại cell kiểm tra Java**\n",
        "3. **Nếu vẫn lỗi, cài đặt lại Java và thiết lập JAVA_HOME**\n",
        "4. **Restart máy tính nếu cần thiết**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative Spark session creation with minimal configuration\n",
        "def create_minimal_spark():\n",
        "    \"\"\"Create Spark session with absolute minimal configuration\"\"\"\n",
        "    try:\n",
        "        # Stop any existing context first\n",
        "        try:\n",
        "            from pyspark import SparkContext\n",
        "            if SparkContext._active_spark_context:\n",
        "                SparkContext._active_spark_context.stop()\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Create with minimal config\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"LeaderBoardAnalysis\") \\\n",
        "            .master(\"local[1]\") \\\n",
        "            .getOrCreate()\n",
        "        \n",
        "        print(\"✅ Minimal Spark session created successfully!\")\n",
        "        return spark\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Minimal Spark session failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Try creating minimal Spark session\n",
        "if spark is None:\n",
        "    print(\"Trying minimal Spark configuration...\")\n",
        "    spark = create_minimal_spark()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'JavaPackage' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session for local mode\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLeaderBoardAnalysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.skewJoin.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.serializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.serializer.KryoSerializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark UI: http://localhost:4040\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\doanh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
            "File \u001b[1;32mc:\\Users\\doanh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[0;32m    637\u001b[0m     ):\n\u001b[0;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session for local mode\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LeaderBoardAnalysis\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: http://localhost:4040\")\n",
        "print(\"Spark session created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data classes for type safety\n",
        "@dataclass\n",
        "class UserData:\n",
        "    uid: str\n",
        "    level: int\n",
        "    team: int\n",
        "    updatedAt: str\n",
        "    name: str\n",
        "    geo: str\n",
        "\n",
        "@dataclass\n",
        "class Score:\n",
        "    id: str\n",
        "    score: float\n",
        "    lastUpdateTime: int\n",
        "    previousScore: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class UserTotalScore:\n",
        "    userId: str\n",
        "    totalScore: float\n",
        "    previousTotalScore: float\n",
        "    lastUpdateTime: int\n",
        "\n",
        "@dataclass\n",
        "class LeaderBoardEntry:\n",
        "    userId: str\n",
        "    totalScore: float\n",
        "    rank: int\n",
        "    lastUpdateTime: int\n",
        "    snapshotTime: int\n",
        "\n",
        "print(\"Data classes defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def parse_timestamp(timestamp_str: str) -> int:\n",
        "    \"\"\"Parse timestamp string to milliseconds\"\"\"\n",
        "    try:\n",
        "        # Try parsing ISO format\n",
        "        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
        "        return int(dt.timestamp() * 1000)\n",
        "    except:\n",
        "        # Fallback to current time\n",
        "        return int(datetime.now().timestamp() * 1000)\n",
        "\n",
        "def format_timestamp(timestamp: int) -> str:\n",
        "    \"\"\"Format timestamp for display\"\"\"\n",
        "    dt = datetime.fromtimestamp(timestamp / 1000, tz=timezone.utc)\n",
        "    return dt.isoformat()\n",
        "\n",
        "print(\"Helper functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Read data from Parquet file\n",
        "input_path = \"app-python/fixed-dataset.parquet\"\n",
        "\n",
        "if os.path.exists(input_path):\n",
        "    print(f\"Reading data from: {input_path}\")\n",
        "    user_data = spark.read.parquet(input_path)\n",
        "    print(f\"Data loaded successfully! Rows: {user_data.count()}\")\n",
        "    user_data.show(10, False)\n",
        "    user_data.printSchema()\n",
        "else:\n",
        "    print(f\"File not found: {input_path}\")\n",
        "    print(\"Available files in app-python directory:\")\n",
        "    if os.path.exists(\"app-python\"):\n",
        "        for file in os.listdir(\"app-python\"):\n",
        "            print(f\"  - {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Transform to Score objects\n",
        "def create_score(row):\n",
        "    timestamp = parse_timestamp(row.updatedAt)\n",
        "    score = float(row.level)\n",
        "    \n",
        "    return {\n",
        "        'id': row.uid,\n",
        "        'score': score,\n",
        "        'lastUpdateTime': timestamp,\n",
        "        'previousScore': None\n",
        "    }\n",
        "\n",
        "scores = user_data.rdd.map(create_score).toDF()\n",
        "print(\"Scores transformed successfully!\")\n",
        "scores.show(10, False)\n",
        "scores.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Calculate total scores in sliding window (5 minutes)\n",
        "window_size_minutes = 5\n",
        "window_size_ms = window_size_minutes * 60 * 1000\n",
        "\n",
        "def calculate_user_total_scores(user_scores):\n",
        "    user_id = user_scores[0]\n",
        "    score_list = sorted(user_scores[1], key=lambda x: x['lastUpdateTime'])\n",
        "    \n",
        "    results = []\n",
        "    for i, current_score in enumerate(score_list):\n",
        "        current_time = current_score['lastUpdateTime']\n",
        "        window_start = current_time - window_size_ms\n",
        "        \n",
        "        # Lấy tất cả scores trong window\n",
        "        scores_in_window = [s for s in score_list if s['lastUpdateTime'] > window_start]\n",
        "        total_score = sum(s['score'] for s in scores_in_window)\n",
        "        \n",
        "        # Tính previous total score\n",
        "        previous_total_score = 0.0\n",
        "        if i > 0:\n",
        "            prev_score = score_list[i-1]\n",
        "            prev_window_start = prev_score['lastUpdateTime'] - window_size_ms\n",
        "            prev_scores_in_window = [s for s in score_list if s['lastUpdateTime'] > prev_window_start]\n",
        "            previous_total_score = sum(s['score'] for s in prev_scores_in_window)\n",
        "        \n",
        "        results.append({\n",
        "            'userId': user_id,\n",
        "            'totalScore': total_score,\n",
        "            'previousTotalScore': previous_total_score,\n",
        "            'lastUpdateTime': current_time\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Group by user và apply function\n",
        "user_scores_rdd = scores.rdd.groupBy(lambda x: x['id']).map(calculate_user_total_scores)\n",
        "total_scores = user_scores_rdd.flatMap(lambda x: x).toDF()\n",
        "\n",
        "print(f\"Total scores calculated for {window_size_minutes} minute window!\")\n",
        "total_scores.show(10, False)\n",
        "print(f\"Total records: {total_scores.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Generate snapshots every 7 minutes\n",
        "def calculate_leaderboard_at_snapshots(total_scores: DataFrame, snapshot_times: List[int], \n",
        "                                     top_n: int, ttl_minutes: int) -> List[LeaderBoardEntry]:\n",
        "    \"\"\"Tính leaderboard tại các snapshot times cụ thể\"\"\"\n",
        "    ttl_ms = ttl_minutes * 60 * 1000\n",
        "    all_snapshots = []\n",
        "    \n",
        "    for snapshot_time in snapshot_times:\n",
        "        cutoff_time = snapshot_time - ttl_ms\n",
        "        \n",
        "        # Lấy tất cả scores hợp lệ tại thời điểm snapshot\n",
        "        valid_scores = total_scores.filter(\n",
        "            (col('lastUpdateTime') <= snapshot_time) & \n",
        "            (col('lastUpdateTime') > cutoff_time)\n",
        "        ).collect()\n",
        "        \n",
        "        # Group by user và lấy score mới nhất cho mỗi user\n",
        "        user_latest_scores = {}\n",
        "        for score in valid_scores:\n",
        "            user_id = score['userId']\n",
        "            if user_id not in user_latest_scores or score['lastUpdateTime'] > user_latest_scores[user_id]['lastUpdateTime']:\n",
        "                user_latest_scores[user_id] = score\n",
        "        \n",
        "        # Sort by total score và lấy top N\n",
        "        sorted_users = sorted(user_latest_scores.values(), key=lambda x: x['totalScore'], reverse=True)\n",
        "        \n",
        "        # Tạo leaderboard entries cho snapshot này\n",
        "        for i, user_score in enumerate(sorted_users[:top_n]):\n",
        "            all_snapshots.append(LeaderBoardEntry(\n",
        "                userId=user_score['userId'],\n",
        "                totalScore=user_score['totalScore'],\n",
        "                rank=i + 1,\n",
        "                lastUpdateTime=user_score['lastUpdateTime'],\n",
        "                snapshotTime=snapshot_time\n",
        "            ))\n",
        "    \n",
        "    return all_snapshots\n",
        "\n",
        "def generate_snapshots(total_scores: DataFrame, top_n: int, ttl_minutes: int, \n",
        "                      snapshot_interval_minutes: int = 7) -> List[LeaderBoardEntry]:\n",
        "    \"\"\"Generate snapshots mỗi 7 phút theo event time\"\"\"\n",
        "    # Lấy tất cả timestamps\n",
        "    all_timestamps = [row['lastUpdateTime'] for row in total_scores.select('lastUpdateTime').distinct().collect()]\n",
        "    all_timestamps.sort()\n",
        "    \n",
        "    if not all_timestamps:\n",
        "        return []\n",
        "    \n",
        "    first_timestamp = all_timestamps[0]\n",
        "    last_timestamp = all_timestamps[-1]\n",
        "    \n",
        "    # Generate snapshot times (mỗi 7 phút)\n",
        "    snapshot_interval_ms = snapshot_interval_minutes * 60 * 1000\n",
        "    snapshot_times = []\n",
        "    \n",
        "    # Snapshot đầu tiên sau 7 phút từ record đầu tiên\n",
        "    first_snapshot_time = first_timestamp + snapshot_interval_ms\n",
        "    current_time = first_snapshot_time\n",
        "    \n",
        "    while current_time <= last_timestamp:\n",
        "        snapshot_times.append(current_time)\n",
        "        current_time += snapshot_interval_ms\n",
        "    \n",
        "    print(f\"Generated {len(snapshot_times)} snapshot times\")\n",
        "    for ts in snapshot_times:\n",
        "        print(f\"  Snapshot time: {format_timestamp(ts)}\")\n",
        "    \n",
        "    # Tính leaderboard tại các snapshot times\n",
        "    return calculate_leaderboard_at_snapshots(total_scores, snapshot_times, top_n, ttl_minutes)\n",
        "\n",
        "print(\"Snapshot functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Generate leaderboard snapshots\n",
        "top_n = 10\n",
        "ttl_minutes = 30\n",
        "snapshot_interval_minutes = 7\n",
        "\n",
        "print(f\"Generating leaderboard snapshots with:\")\n",
        "print(f\"  Top N: {top_n}\")\n",
        "print(f\"  TTL: {ttl_minutes} minutes\")\n",
        "print(f\"  Snapshot interval: {snapshot_interval_minutes} minutes\")\n",
        "\n",
        "snapshots = generate_snapshots(\n",
        "    total_scores, \n",
        "    top_n, \n",
        "    ttl_minutes, \n",
        "    snapshot_interval_minutes\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated {len(snapshots)} leaderboard entries across snapshots.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Display results\n",
        "if snapshots:\n",
        "    # Convert to DataFrame for better display\n",
        "    snapshot_data = []\n",
        "    for entry in snapshots:\n",
        "        snapshot_data.append({\n",
        "            'userId': entry.userId,\n",
        "            'totalScore': entry.totalScore,\n",
        "            'rank': entry.rank,\n",
        "            'lastUpdateTime': entry.lastUpdateTime,\n",
        "            'snapshotTime': entry.snapshotTime,\n",
        "            'snapshotTimeFormatted': format_timestamp(entry.snapshotTime),\n",
        "            'lastUpdateTimeFormatted': format_timestamp(entry.lastUpdateTime)\n",
        "        })\n",
        "    \n",
        "    snapshot_df = spark.createDataFrame(snapshot_data)\n",
        "    \n",
        "    print(\"\\n=== LEADERBOARD SNAPSHOTS ===\")\n",
        "    snapshot_df.orderBy(\"snapshotTime\", \"rank\").show(50, False)\n",
        "    \n",
        "    # Show summary by snapshot time\n",
        "    print(\"\\n=== SNAPSHOT SUMMARY ===\")\n",
        "    snapshot_summary = snapshot_df.groupBy(\"snapshotTimeFormatted\") \\\n",
        "        .agg(\n",
        "            count(\"userId\").alias(\"userCount\"),\n",
        "            max(\"totalScore\").alias(\"maxScore\"),\n",
        "            min(\"totalScore\").alias(\"minScore\"),\n",
        "            avg(\"totalScore\").alias(\"avgScore\")\n",
        "        ) \\\n",
        "        .orderBy(\"snapshotTimeFormatted\")\n",
        "    \n",
        "    snapshot_summary.show(20, False)\n",
        "    \n",
        "    # Show top users across all snapshots\n",
        "    print(\"\\n=== TOP USERS ACROSS ALL SNAPSHOTS ===\")\n",
        "    top_users = snapshot_df.groupBy(\"userId\") \\\n",
        "        .agg(\n",
        "            count(\"rank\").alias(\"snapshotCount\"),\n",
        "            max(\"totalScore\").alias(\"maxScore\"),\n",
        "            avg(\"totalScore\").alias(\"avgScore\"),\n",
        "            min(\"rank\").alias(\"bestRank\")\n",
        "        ) \\\n",
        "        .orderBy(desc(\"maxScore\"), desc(\"avgScore\"))\n",
        "    \n",
        "    top_users.show(20, False)\n",
        "    \n",
        "else:\n",
        "    print(\"No snapshots generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Save results to file\n",
        "output_path = \"spark-jobs/result/leaderboard_snapshots\"\n",
        "\n",
        "if snapshots:\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    # Write as JSON with partitioning by snapshot time\n",
        "    snapshot_df.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .partitionBy(\"snapshotTime\") \\\n",
        "        .json(output_path)\n",
        "    \n",
        "    print(f\"Snapshots saved to: {output_path}\")\n",
        "    \n",
        "    # Also save as CSV for easier viewing\n",
        "    csv_path = \"spark-jobs/result/leaderboard_snapshots.csv\"\n",
        "    snapshot_df.coalesce(1).write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(csv_path)\n",
        "    \n",
        "    print(f\"CSV version saved to: {csv_path}\")\n",
        "else:\n",
        "    print(\"No snapshots to save!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Performance analysis\n",
        "print(\"\\n=== PERFORMANCE ANALYSIS ===\")\n",
        "print(f\"Total user data records: {user_data.count()}\")\n",
        "print(f\"Total score records: {scores.count()}\")\n",
        "print(f\"Total calculated scores: {total_scores.count()}\")\n",
        "print(f\"Total leaderboard entries: {len(snapshots)}\")\n",
        "\n",
        "if snapshots:\n",
        "    unique_users = len(set(entry.userId for entry in snapshots))\n",
        "    unique_snapshots = len(set(entry.snapshotTime for entry in snapshots))\n",
        "    \n",
        "    print(f\"Unique users in leaderboard: {unique_users}\")\n",
        "    print(f\"Unique snapshot times: {unique_snapshots}\")\n",
        "    \n",
        "    # Calculate average users per snapshot\n",
        "    avg_users_per_snapshot = len(snapshots) / unique_snapshots if unique_snapshots > 0 else 0\n",
        "    print(f\"Average users per snapshot: {avg_users_per_snapshot:.2f}\")\n",
        "\n",
        "print(\"\\n=== ANALYSIS COMPLETED ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "print(\"Stopping Spark session...\")\n",
        "spark.stop()\n",
        "print(\"Spark session stopped successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
